%% PPGCC - CCN - UFPI

%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/
%% Exemplo de arquivo .bib a ser utilizado para gerar a Biliografia

%% Quando for no Google Scholar, clique em bibtex na citação e copie o código pra colar aqui





@misc{abntex2-wiki-como-customizar,
  author        = {abnTeX2},
  date-added    = {2013-03-23 21:39:21 +0000},
  date-modified = {2013-03-23 21:44:20 +0000},
  howpublished  = {Wiki do abnTeX2},
  keywords      = {wiki},
  title         = {Como customizar o abnTeX2},
  url           = {https://code.google.com/p/abntex2/wiki/ComoCustomizar},
  urlaccessdate = {23 mar. 2013},
  year          = {2013},
  bdsk-url-1    = {https://code.google.com/p/abntex2/wiki/ComoCustomizar}
}

@manual{abntex2cite,
  annote        = {Este documento {\'e} derivado do \cite{abnt-bibtex-doc}},
  author        = {abnTeX2 and Lauro C{\'e}sar Araujo},
  date-added    = {2013-01-09 10:37:45 +0000},
  date-modified = {2013-04-05 10:47:36 +0000},
  organization  = {Equipe abnTeX2},
  title         = {O pacote abntex2cite: Estilos bibliogr{\'a}ficos compat{\'\i}veis com a ABNT NBR 6023},
  url           = {http://abntex2.googlecode.com/},
  year          = {2013},
  bdsk-url-1    = {http://code.google.com/p/abntex2/}
}

@manual{abntex2cite-alf,
  annote        = {Este documento {\'e} derivado do \cite{abnt-bibtex-alf-doc}},
  author        = {abnTeX2 and Lauro C{\'e}sar Araujo},
  date-added    = {2013-01-09 10:37:45 +0000},
  date-modified = {2013-04-05 11:03:05 +0000},
  organization  = {Equipe abnTeX2},
  title         = {O pacote abntex2cite: t{\'o}picos espec{\'\i}ficos da ABNT NBR 10520:2002 e o estilo bibliogr{\'a}fico alfab{\'e}tico (sistema autor-data)},
  url           = {http://abntex2.googlecode.com/},
  year          = {2013},
  bdsk-url-1    = {http://code.google.com/p/abntex2/}
}

@manual{abntex2classe,
  author        = {abnTeX2 and Lauro C{\'e}sar Araujo},
  date-added    = {2013-01-09 10:37:38 +0000},
  date-modified = {2013-04-05 11:03:48 +0000},
  organization  = {Equipe abnTeX2},
  title         = {A classe abntex2: Modelo can{\^o}nico de trabalhos acad{\^e}micos brasileiros compat{\'\i}vel com as normas ABNT NBR 14724:2011, ABNT NBR 6024:2012 e outras},
  url           = {http://abntex2.googlecode.com/},
  year          = {2013},
  bdsk-url-1    = {http://code.google.com/p/abntex2/}
}

@manual{abntex2modelo,
  annote        = {Este documento {\'e} derivado do \cite{abnt-bibtex-doc}},
  author        = {abnTeX2},
  date-added    = {2013-01-12 22:55:32 +0000},
  date-modified = {2013-02-04 12:05:54 +0000},
  organization  = {Equipe abnTeX2},
  title         = {Modelo Can{\^o}nico de Trabalho Acad{\^e}mico com abnTeX2},
  url           = {http://abntex2.googlecode.com/},
  year          = {2013},
  bdsk-url-1    = {http://code.google.com/p/abntex2/}
}

@manual{abntex2modelo-artigo,
  annote        = {Este documento {\'e} derivado do \cite{abnt-bibtex-doc}},
  author        = {abnTeX2},
  date-added    = {2013-01-15 00:10:35 +0000},
  date-modified = {2013-02-04 12:05:47 +0000},
  organization  = {Equipe abnTeX2},
  title         = {Modelo Can{\^o}nico de Artigo Cient{\'\i}fico com abnTeX2},
  url           = {http://abntex2.googlecode.com/},
  year          = {2013},
  bdsk-url-1    = {http://code.google.com/p/abntex2/}
}

@manual{abntex2modelo-relatorio,
  annote        = {Este documento {\'e} derivado do \cite{abnt-bibtex-doc}},
  author        = {abnTeX2},
  date-added    = {2013-01-15 00:05:34 +0000},
  date-modified = {2013-02-04 12:05:50 +0000},
  organization  = {Equipe abnTeX2},
  title         = {Modelo Can{\^o}nico de Relat{\'o}rio T{\'e}cnico e/ou Cient{\'\i}fico com abnTeX2},
  url           = {http://abntex2.googlecode.com/},
  year          = {2013},
  bdsk-url-1    = {http://code.google.com/p/abntex2/}
}

@mastersthesis{araujo2012,
  address       = {Bras{\'\i}lia},
  author        = {Lauro C{\'e}sar Araujo},
  date-added    = {2013-01-09 11:04:42 +0000},
  date-modified = {2013-01-09 11:04:42 +0000},
  month         = {mar.},
  school        = {Universidade de Bras{\'\i}lia},
  subtitle      = {uma perspectiva de {A}rquitetura da {I}nforma{\c c}{\~a}o da {E}scola de {B}ras{\'\i}lia},
  title         = {Configura{\c c}{\~a}o},
  year          = {2012}
}

@manual{babel,
  author        = {Johannes Braams},
  date-added    = {2013-02-17 13:37:14 +0000},
  date-modified = {2013-02-17 13:38:38 +0000},
  month         = {Apr.},
  title         = {Babel, a multilingual package for use with LATEX's standard document classes},
  url           = {http://mirrors.ctan.org/info/babel/babel.pdf},
  urlaccessdate = {17 fev. 2013},
  year          = {2008},
  bdsk-url-1    = {http://mirrors.ctan.org/info/babel/babel.pdf}
}

@incollection{bates2010,
  address       = {New York},
  author        = {Marcia J. Bates},
  booktitle     = {Encyclopedia of Library and Information Sciences},
  date-added    = {2012-04-23 11:34:29 +0000},
  date-modified = {2012-04-23 11:34:29 +0000},
  edition       = {3rd},
  editor        = {Marcia J. Bates and Mary Niles Maack},
  pages         = {2347-2360},
  publisher     = {CRC Press},
  title         = {Information},
  url           = {http://pages.gseis.ucla.edu/faculty/bates/articles/information.html},
  urlaccessdate = {24 out. 2011},
  volume        = {3},
  year          = {2010},
  bdsk-url-1    = {http://pages.gseis.ucla.edu/faculty/bates/articles/information.html}
}

@book{dewey1980,
  address       = {New York, NY, USA},
  author        = {John Dewey},
  date-added    = {2012-04-23 11:34:16 +0000},
  date-modified = {2012-04-23 11:34:16 +0000},
  publisher     = {Perigee Books},
  title         = {Art as Experience},
  year          = {1980}
}

@book{doxiadis1965,
  author        = {Constantinos A. Doxiadis},
  date-added    = {2012-04-23 11:34:20 +0000},
  date-modified = {2012-04-23 11:34:20 +0000},
  publisher     = {Ceira - Coimbra},
  title         = {Arquitetura em Transi{\c c}{\~a}o},
  year          = {1965}
}

@manual{EIA649B,
  address       = {EUA},
  date-added    = {2012-04-23 11:34:59 +0000},
  date-modified = {2012-04-23 11:34:59 +0000},
  keywords      = {norma},
  month         = {June},
  organization  = {TechAmerica},
  title         = {ANSI/EIA 649-B: Configuration Management Standard},
  year          = {2011}
}

@inbook{guarino1995,
  address       = {Vienna},
  author        = {Nicola Guarino},
  booktitle     = {Philosophy and the Cognitive Science},
  date-added    = {2012-04-23 11:34:29 +0000},
  date-modified = {2012-04-23 11:34:29 +0000},
  editor        = {R. Casati and B. Smith and G. White},
  month         = {Sept.},
  pages         = {443-456},
  publisher     = {Holder-Pivhler-Tempsky},
  title         = {The Ontological Level},
  url           = {http://wiki.loa-cnr.it/Papers/OntLev.pdf},
  urlaccessdate = {2 jan. 2012},
  year          = {1995},
  bdsk-url-1    = {http://wiki.loa-cnr.it/Papers/OntLev.pdf}
}

@phdthesis{guizzardi2005,
  address       = {Enschede, The Netherlands},
  author        = {Giancarlo Guizzardi},
  date-added    = {2012-04-23 11:35:28 +0000},
  date-modified = {2012-04-23 11:35:28 +0000},
  school        = {Centre for Telematics and Information Technology, University of Twente},
  title         = {Ontological Foundations for Structural Conceptual Models},
  url           = {http://www.loa.istc.cnr.it/Guizzardi/SELMAS-CR.pdf},
  urlaccessdate = {3 jul. 2011},
  year          = {2005},
  bdsk-url-1    = {http://www.loa.istc.cnr.it/Guizzardi/SELMAS-CR.pdf}
}

@book{ibge1993,
  address       = {Rio de Janeiro},
  author        = {IBGE},
  date-added    = {2013-08-21 13:56:10 +0000},
  date-modified = {2013-08-21 13:56:10 +0000},
  edition       = {3},
  organization  = {http://biblioteca.ibge.gov.br/visualizacao/livros/liv23907.pdf},
  publisher     = {Centro de Documenta{\c c}{\~a}o e Dissemina{\c c}{\~a}o de Informa{\c c}{\~o}es. Funda{\c c}{\~a}o Intituto Brasileiro de Geografia e Estat{\'\i}stica},
  title         = {Normas de apresenta{\c c}{\~a}o tabular},
  urlaccessdate = {21 ago 2013},
  year          = {1993}
}

@mastersthesis{macedo2005,
  author        = {Fl{\'a}via L. Macedo},
  date-added    = {2012-04-23 11:35:13 +0000},
  date-modified = {2012-04-23 11:35:13 +0000},
  keywords      = {arquitetura da informa{\c c}{\~a}o},
  school        = {Universidade de Bras{\'\i}lia},
  title         = {Arquitetura da Informa{\c c}{\~a}o: aspectos espistemol{\'o}gicos, cient{\'\i}ficos e pr{\'a}ticos.},
  type          = {Disserta{\c c}{\~a}o de Mestrado},
  year          = {2005}
}

@inproceedings{masolo2010,
  author        = {Claudio Masolo},
  booktitle     = {Proceedings of the Twelfth International Conference on the Principles of Knowledge Representation and Reasoning (KR 2010)},
  date-added    = {2012-04-23 11:34:38 +0000},
  date-modified = {2012-04-23 11:34:38 +0000},
  editor        = {Lin, F. and Sattler, U.},
  pages         = {258-268},
  publisher     = {AAAI Press},
  title         = {Understanding Ontological Levels},
  url           = {http://wiki.loa-cnr.it/Papers/kr10v0.7.pdf},
  urlaccessdate = {2 jan. 2012},
  year          = {2010},
  bdsk-url-1    = {http://wiki.loa-cnr.it/Papers/kr10v0.7.pdf}
}

@manual{memoir,
  address       = {Normandy Park, WA},
  author        = {Peter Wilson and Lars Madsen},
  date-added    = {2013-01-09 10:37:50 +0000},
  date-modified = {2013-03-21 13:23:25 +0000},
  organization  = {The Herries Press},
  title         = {The Memoir Class for Configurable Typesetting - User Guide},
  url           = {http://mirrors.ctan.org/macros/latex/contrib/memoir/memman.pdf},
  urlaccessdate = {19 dez. 2012},
  year          = {2010},
  bdsk-url-1    = {http://ctan.tche.br/macros/latex/contrib/memoir/memman.pdf}
}

@manual{NBR10520:2002,
  address       = {Rio de Janeiro},
  date-added    = {2012-12-15 21:43:38 +0000},
  date-modified = {2013-01-12 22:17:20 +0000},
  month         = {ago.},
  org-short     = {ABNT},
  organization  = {Associa{\c c}\~ao Brasileira de Normas T\'ecnicas},
  pages         = 7,
  subtitle      = {Informa{\c c}\~ao e documenta{\c c}\~ao --- Apresenta{\c c}\~ao de cita{\c c}\~oes em documentos},
  title         = {{NBR} 10520},
  year          = 2002
}

@manual{NBR14724:2001,
  address       = {Rio de Janeiro},
  date-added    = {2012-12-15 20:34:08 +0000},
  date-modified = {2012-12-15 20:34:08 +0000},
  month         = {jul.},
  org-short     = {ABNT},
  organization  = {Associa{\c c}\~ao Brasileira de Normas T\'ecnicas},
  pages         = 6,
  subtitle      = {Informa{\c c}\~ao e documenta{\c c}\~ao --- trabalhos acad\^emicos --- apresenta{\c c}\~ao},
  title         = {{NBR} 14724},
  year          = 2001
}

@manual{NBR14724:2002,
  address       = {Rio de Janeiro},
  date-added    = {2012-12-15 20:34:17 +0000},
  date-modified = {2012-12-15 20:34:17 +0000},
  month         = {ago.},
  org-short     = {ABNT},
  organization  = {Associa{\c c}\~ao Brasileira de Normas T\'ecnicas},
  pages         = 6,
  subtitle      = {Informa{\c c}\~ao e documenta{\c c}\~ao --- trabalhos acad\^emicos --- apresenta{\c c}\~ao},
  title         = {{NBR} 14724},
  year          = 2002
}

@manual{NBR14724:2005,
  address       = {Rio de Janeiro},
  date-added    = {2012-12-15 20:34:08 +0000},
  date-modified = {2012-12-15 20:35:25 +0000},
  month         = {dez.},
  org-short     = {ABNT},
  organization  = {Associa{\c c}\~ao Brasileira de Normas T\'ecnicas},
  pages         = 9,
  subtitle      = {Informa{\c c}\~ao e documenta{\c c}\~ao --- trabalhos acad\^emicos --- apresenta{\c c}\~ao},
  title         = {{NBR} 14724},
  year          = 2005
}

@manual{NBR14724:2011,
  address       = {Rio de Janeiro},
  date-added    = {2012-12-15 20:34:08 +0000},
  date-modified = {2012-12-15 20:35:25 +0000},
  month         = {mar.},
  note          = {Substitui a Ref.~\citeonline{NBR14724:2005}},
  org-short     = {ABNT},
  organization  = {Associa{\c c}\~ao Brasileira de Normas T\'ecnicas},
  pages         = 15,
  subtitle      = {Informa{\c c}\~ao e documenta{\c c}\~ao --- trabalhos acad\^emicos --- apresenta{\c c}\~ao},
  title         = {{NBR} 14724},
  year          = 2011
}

@manual{NBR6024:2012,
  address       = {Rio de Janeiro},
  date-added    = {2012-12-15 21:24:06 +0000},
  date-modified = {2012-12-15 21:24:28 +0000},
  month         = {fev.},
  org-short     = {ABNT},
  organization  = {Associa{\c c}\~ao Brasileira de Normas T\'ecnicas},
  pages         = 4,
  subtitle      = {Numera{\c c}\~ao progressiva das se{\c c}\~oes de um documento},
  title         = {{NBR} 6024},
  year          = 2012
}

@manual{NBR6028:2003,
  address       = {Rio de Janeiro},
  date-added    = {2012-12-15 21:02:12 +0000},
  date-modified = {2012-12-15 21:02:50 +0000},
  month         = {nov.},
  org-short     = {ABNT},
  organization  = {Associa{\c c}\~ao Brasileira de Normas T\'ecnicas},
  pages         = 2,
  subtitle      = {Resumo - Apresenta{\c c}{\~a}o},
  title         = {{NBR} 6028},
  year          = 2003
}

@manual{talbot2012,
  author        = {Nicola L.C. Talbot},
  date-added    = {2013-03-11 12:06:04 +0000},
  date-modified = {2013-03-11 12:06:56 +0000},
  month         = {Nov.},
  title         = {User Manual for glossaries.sty},
  url           = {http://mirrors.ctan.org/macros/latex/contrib/glossaries/glossaries-user.pdf},
  urlaccessdate = {11 mar. 2013},
  year          = {2012},
  bdsk-url-1    = {http://mirrors.ctan.org/macros/latex/contrib/glossaries/glossaries-user.pdf}
}

@article{van86,
  author  = {{van}, Gigch, John P. and Leo L. Pipino},
  journal = {Future Computing Systems},
  number  = {1},
  pages   = {71-97},
  title   = {In search for a paradigm for the discipline of information systems},
  volume  = {1},
  year    = {1986}
}

@techreport{StephenMarsland2014,
  author = {Stephen Marsland},
  city   = {New York},
  doi    = {10.1201/b17476},
  isbn   = {9780429102509},
  month  = {10},
  pages  = {4-5},
  title  = {Machine Learning - An Algorithmic Perspective},
  year   = {2014}
}

@book{SindhuMeena2020,
  abstract  = {High dimensionality would be one of the major challenges faced by people working in research with big data as a high dimensionality that happens, while a dataset comprises of a big number of features. For resolving this issue, often researchers make use of a feature selection step for identification and removal of irrelevant features and repetitive features. Acceleration Artificial Bee Colony-Artificial Neural Network (AABC-ANN) has been introduced in the preceding research for handling the feature selection process over the big data. Computational complexity and inaccuracy of dataset remain as a problem for these methods. Enhanced Particle Swarm Optimization with Genetic Algorithm – Modified Artificial Neural Network (EPSOGA–MANN) is described in the proposed methodology for avoiding the above-mentioned issues. Modules including preprocessing, feature selection, and classification have been included in this research process. Fuzzy C Means (FCM) denotes the clustering algorithm which is used to handle the noise information efficiently in preprocessing. Feature selection process is carried out by means of EPSOGA algorithm optimally in this research. More important and relevant features are selected by EPSOGA optimization algorithm and as a result more accurate classification results are achieved in this work for huge volume of dataset. Input, hidden, and output layers are the three layers of MANN. It is introduced for improving the time complexity by means of neurons. The performance evaluation of the research method is conducted in the Matlab simulation environment.},
  author    = {K., Suriya, S. Sindhu Meena},
  doi       = {10.1007/978-3-030-24051-6},
  journal   = {Proceedings of International Conference on Artificial Intelligence, Smart Grid and Smart City Applications},
  month     = {3},
  publisher = {Springer International Publishing},
  title     = {Proceedings of International Conference on Artificial Intelligence, Smart Grid and Smart City Applications},
  year      = {2020}
}

@article{Dike2018,
  abstract  = {Artificial neural networks (ANN) have been applied effectively in numerous fields for the aim of prediction, knowledge discovery, classification, time series analysis, modeling, etc. ANN training can be assorted into Supervised learning, Reinforcement learning and Unsupervised learning. There are some limitations using supervised learning. These limitations can be overcome by using unsupervised learning technique. This gives us motivation to write a review on unsupervised learning based on ANN. One main problem associated with unsupervised learning is how to find the hidden structures in unlabeled data. This paper reviews on the training/learning of unsupervised learning based on artificial neural network. It provides a description of the methods of selecting and fixing a number of hidden nodes in an unsupervised learning environment based on ANN. Moreover, the status, benefits and challenges of unsupervised learning are also summarized.},
  author    = {Happiness Ugochi Dike and Yimin Zhou and Kranthi Kumar Deveerasetty and Qingtian Wu},
  doi       = {10.1109/CBS.2018.8612259},
  isbn      = {9781538673553},
  journal   = {2018 IEEE International Conference on Cyborg and Bionic Systems, CBS 2018},
  keywords  = {Artificial Neural Network,Hidden nodes,Training,Unsupervised learning},
  month     = {7},
  pages     = {322-327},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {Unsupervised Learning Based On Artificial Neural Network: A Review},
  year      = {2018}
}

@article{Gaudreault2021,
  abstract  = {Numerous machine learning applications involve dealing with imbalanced domains, where the learning focus is on the least frequent classes. This imbalance introduces new challenges for both the performance assessment of these models and their predictive modeling. While several performance metrics have been established as baselines in balanced domains, some cannot be applied to the imbalanced case since the use of the majority class in the metric could lead to a misleading evaluation of performance. Other metrics, such as the area under the precision-recall curve, have been demonstrated to be more appropriate for imbalance domains due to their focus on class-specific performance. There are, however, many proposed implementations for this particular metric, which could potentially lead to different conclusions depending on the one used. In this research, we carry out an experimental study to better understand these issues and aim at providing a set of recommendations by studying the impact of using different metrics and different implementations of the same metric under multiple imbalance settings.},
  author    = {Jean Gabriel Gaudreault and Paula Branco and João Gama},
  doi       = {10.1007/978-3-030-88942-5_6/TABLES/6},
  isbn      = {9783030889418},
  issn      = {16113349},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords  = {Imbalanced domains,Performance evaluation,Performance metrics,Precision-recall curve},
  pages     = {67-77},
  publisher = {Springer Science and Business Media Deutschland GmbH},
  title     = {An Analysis of Performance Metrics for Imbalanced Classification},
  volume    = {12986 LNAI},
  url       = {https://link.springer.com/chapter/10.1007/978-3-030-88942-5_6},
  year      = {2021}
}

@article{Canbek2023,
  abstract  = {Although few performance evaluation instruments have been used conventionally in different machine learning-based classification problem domains, there are numerous ones defined in the literature. This study reviews and describes performance instruments via formally defined novel concepts and clarifies the terminology. The study first highlights the issues in performance evaluation via a survey of 78 mobile-malware classification studies and reviews terminology. Based on three research questions, it proposes novel concepts to identify characteristics, similarities, and differences of instruments that are categorized into ‘performance measures’ and ‘performance metrics’ in the classification context for the first time. The concepts reflecting the intrinsic properties of instruments such as canonical form, geometry, duality, complementation, dependency, and leveling, aim to reveal similarities and differences of numerous instruments, such as redundancy and ground-truth versus prediction focuses. As an application of knowledge representation, we introduced a new exploratory table called PToPI (Periodic Table of Performance Instruments) for 29 measures and 28 metrics (69 instruments including variant and parametric ones). Visualizing proposed concepts, PToPI provides a new relational structure for the instruments including graphical, probabilistic, and entropic ones to see their properties and dependencies all in one place. Applications of the exploratory table in six examples from different domains in the literature have shown that PToPI aids overall instrument analysis and selection of the proper performance metrics according to the specific requirements of a classification problem. We expect that the proposed concepts and PToPI will help researchers comprehend and use the instruments and follow a systematic approach to classification performance evaluation and publication.},
  author    = {Gürol Canbek and Tugba Taskaya Temizel and Seref Sagiroglu},
  doi       = {10.1007/S42979-022-01409-1/TABLES/2},
  issn      = {26618907},
  issue     = {1},
  journal   = {SN Computer Science},
  keywords  = {Classification,Knowledge representation,Machine learning,Performance evaluation,Performance measures,Performance metrics,Periodic table},
  month     = {1},
  pages     = {1-30},
  publisher = {Springer},
  title     = {PToPI: A Comprehensive Review, Analysis, and Knowledge Representation of Binary Classification Performance Measures/Metrics},
  volume    = {4},
  url       = {https://link.springer.com/article/10.1007/s42979-022-01409-1},
  year      = {2023}
}

@article{Ferri2009,
  abstract  = {Performance metrics in classification are fundamental in assessing the quality of learning methods and learned models. However, many different measures have been defined in the literature with the aim of making better choices in general or for a specific application area. Choices made by one metric are claimed to be different from choices made by other metrics. In this work, we analyse experimentally the behaviour of 18 different performance metrics in several scenarios, identifying clusters and relationships between measures. We also perform a sensitivity analysis for all of them in terms of several traits: class threshold choice, separability/ranking quality, calibration performance and sensitivity to changes in prior class distribution. From the definitions and experiments, we make a comprehensive analysis of the relationships between metrics, and a taxonomy and arrangement of them according to the previous traits. This can be useful for choosing the most adequate measure (or set of measures) for a specific application. Additionally, the study also highlights some niches in which new measures might be defined and also shows that some supposedly innovative measures make the same choices (or almost) as existing ones. Finally, this work can also be used as a reference for comparing experimental results in pattern recognition and machine learning literature, when using different measures.},
  author    = {C. Ferri and J. Hernández-Orallo and R. Modroiu},
  doi       = {10.1016/J.PATREC.2008.08.010},
  issn      = {0167-8655},
  issue     = {1},
  journal   = {Pattern Recognition Letters},
  month     = {1},
  pages     = {27-38},
  publisher = {North-Holland},
  title     = {An experimental comparison of performance measures for classification},
  volume    = {30},
  url       = {https://www.sciencedirect.com/science/article/abs/pii/S0167865508002687},
  year      = {2009}
}

@article{Seiffert2008,
  abstract = {Boosting has been shown to improve the performance of classifiers in many situations, including when data is im-balanced. There are, however, two possible implementations of boosting, and it is unclear which should be used. Boosting by reweighting is typically used, but can only be applied to base learners which are designed to handle example weights. On the other hand, boosting by resampling can be applied to any base learner. In this work, we empirically evaluate the differences between these two boosting implementations using imbalanced training data. Using 10 boosting algorithms, 4 learners and 15 datasets, we find that boosting by resampling performs as well as, or significantly better than, boosting by reweighting (which is often the default boosting implementation). We therefore conclude that in general, boosting by resampling is preferred over boosting by weighting. ©2008 IEEE.},
  author   = {Chris Seiffert and Taghi M. Khoshgoftaar and Jason Van Hulse and Amri Napolitano},
  doi      = {10.1109/ICTAI.2008.59},
  isbn     = {9780769534404},
  issn     = {10823409},
  journal  = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
  pages    = {445-451},
  title    = {Resampling or reweighting: A comparison of boosting implementations},
  volume   = {1},
  year     = {2008}
}

@article{Batuwita2010,
  abstract  = {Random undersampling and oversampling are simple but well-known resampling methods applied to solve the problem of class imbalance. In this paper we show that the random oversampling method can produce better classification results than the random undersampling method, since the oversampling can increase the minority class recognition rate by sacrificing less amount of majority class recognition rate than the undersampling method. However, the random oversampling method would increase the computational cost associated with the SVM training largely due to the addition of new training examples. In this paper we present an investigation carried out to develop efficient resampling methods that can produce comparable classification results to the random oversampling results, but with the use of less amount of data. The main idea of the proposed methods is to first select the most informative data examples located closer to the class boundary region by using the separating hyperplane found by training an SVM model on the original imbalanced dataset, and then use only those examples in resampling. We demonstrate that it would be possible to obtain comparable classification results to the random oversampling results through two sets of efficient resampling methods which use 50% less amount of data and 75% less amount of data, respectively, compared to the sizes of the datasets generated by the random oversampling method. © 2010 IEEE.},
  author    = {Rukshan Batuwita and Vasile Palade},
  doi       = {10.1109/IJCNN.2010.5596787},
  isbn      = {9781424469178},
  journal   = {Proceedings of the International Joint Conference on Neural Networks},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {Efficient resampling methods for training support vector machines with imbalanced datasets},
  year      = {2010}
}

@article{Namvar2018,
  abstract  = {Credit risk prediction is an effective way of evaluating whether a potential borrower will repay a loan, particularly in peer-to-peer lending where class imbalance problems are prevalent. However, few credit risk prediction models for social lending consider imbalanced data and, further, the best resampling technique to use with imbalanced data is still controversial. In an attempt to address these problems, this paper presents an empirical comparison of various combinations of classifiers and resampling techniques within a novel risk assessment methodology that incorporates imbalanced data. The credit predictions from each combination are evaluated with a G-mean measure to avoid bias towards the majority class, which has not been considered in similar studies. The results reveal that combining random forest and random under-sampling may be an effective strategy for calculating the credit risk associated with loan applicants in social lending markets.},
  author    = {Anahita Namvar and Mohammad Siami and Fethi Rabhi and Mohsen Naderpour},
  doi       = {10.2991/IJCIS.11.1.70/METRICS},
  issn      = {18756883},
  issue     = {1},
  journal   = {International Journal of Computational Intelligence Systems},
  keywords  = {Imbalance classification,Peer-to-peer lending,Resampling,Risk prediction},
  month     = {3},
  pages     = {925-935},
  publisher = {Atlantis Press},
  title     = {Credit risk prediction in an imbalanced social lending environment},
  volume    = {11},
  url       = {https://link-springer-com.ez42.periodicos.capes.gov.br/article/10.2991/ijcis.11.1.70},
  year      = {2018}
}

@article{Chakravarthy2019,
  abstract  = {Class imbalance is a problem of crucial challenge in many real-world machine learning applications. Traditional machine learning algorithms are likely to produce good accuracy scores on such datasets due to an obvious bias towards the majority class. Thus, accuracy as a measure of performance for algorithms working on imbalanced data is not very clearly defined since the classifier has poor predictive accuracy over the minority class. While previous work has used several resampling techniques to aid in improving the predictive accuracy of the minority class, in this study, we explore and compare the effectiveness of the Synthetic Minority Oversampling and Random Oversampling techniques over multiple learning algorithms and resampling ratios for eight different performance measures against two datasets from diverse domains such as medicine and engineering. The results of this study show that the effectiveness of these resampling techniques is a multivariate function relative to both the learning algorithms and the resampling ratios, as well as the coherent characteristics of datasets. The choice of performance measures to evaluate models built using these resampling techniques also vary, thus giving us more relevant information useful for future research and applications.},
  author    = {Adithi D. Chakravarthy and Sindhura Bonthu and Zhengxin Chen and Qiuming Zhu},
  doi       = {10.1109/ICMLA.2019.00245},
  isbn      = {9781728145495},
  journal   = {Proceedings - 18th IEEE International Conference on Machine Learning and Applications, ICMLA 2019},
  keywords  = {Class-imbalance,Classification,Oversampling,Predictive-models,Resampling,Undersampling},
  month     = {12},
  pages     = {1492-1495},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {Predictive models with resampling: A comparative study of machine learning algorithms and their performances on handling imbalanced datasets},
  year      = {2019}
}

@article{Carvalho2025,
  abstract  = {This article presents a data-driven review of resampling approaches aimed at mitigating the class imbalance problem in machine learning, a widespread issue that limits classifier performance across numerous sectors. Initially, this research provides an extensive theoretical examination of the class imbalance problem, emphasizing its propensity to amplify existing data difficulty factors, including class overlap, small disjuncts, and noise, thus biasing the model towards the majority class. Acknowledging the significance of detecting and quantifying the synergistic effects between class imbalance and these data difficulty factors, this study surveys metrics formulated to quantify such phenomena in imbalanced domains. Subsequently, an exhaustive review of recent oversampling, undersampling, and hybrid sampling approaches is conducted. A major finding arising from this review is the discernible shift in resampling approaches towards enhanced adaptability. This is achieved through the identification of problematic regions and the subsequent implementation of customized resampling protocols. Concurrently, a methodological divergence is observed in both oversampling and undersampling strategies: certain oversampling methods target regions of higher classification complexity, which are crucial for effective model training, while others focus on areas of lower classification complexity to safely oversample the minority class. In contrast, undersampling approaches either predominantly remove majority samples from redundant regions or focus on class boundaries to reduce class overlap. However, despite this increased adaptability, no resampling method consistently demonstrated superior performance across all documented experiments. Consequently, we explore a promising strategy, namely the adoption of recommendation systems for resampling approaches. Lastly, the primary research challenges within this topic are discussed.},
  author    = {Miguel Carvalho and Armando J. Pinho and Susana Brás},
  doi       = {10.1186/S40537-025-01119-4},
  issn      = {2196-1115},
  issue     = {1},
  journal   = {Journal of Big Data 2025 12:1},
  keywords  = {Communications Engineering,Computational Science and Engineering,Data Mining and Knowledge Discovery,Database Management,Information Storage and Retrieval,Mathematical Applications in Computer Science,Networks,Resampling approaches,SMOTE},
  month     = {3},
  pages     = {1-58},
  publisher = {SpringerOpen},
  title     = {Resampling approaches to handle class imbalance: a review from a data perspective},
  volume    = {12},
  url       = {https://link-springer-com.ez42.periodicos.capes.gov.br/articles/10.1186/s40537-025-01119-4 https://link-springer-com.ez42.periodicos.capes.gov.br/article/10.1186/s40537-025-01119-4},
  year      = {2025}
}

@article{Haixiang2017,
  abstract  = {Rare events, especially those that could potentially negatively impact society, often require humans’ decision-making responses. Detecting rare events can be viewed as a prediction task in data mining and machine learning communities. As these events are rarely observed in daily life, the prediction task suffers from a lack of balanced data. In this paper, we provide an in depth review of rare event detection from an imbalanced learning perspective. Five hundred and seventeen related papers that have been published in the past decade were collected for the study. The initial statistics suggested that rare events detection and imbalanced learning are concerned across a wide range of research areas from management science to engineering. We reviewed all collected papers from both a technical and a practical point of view. Modeling methods discussed include techniques such as data preprocessing, classification algorithms and model evaluation. For applications, we first provide a comprehensive taxonomy of the existing application domains of imbalanced learning, and then we detail the applications for each category. Finally, some suggestions from the reviewed papers are incorporated with our experiences and judgments to offer further research directions for the imbalanced learning and rare event detection fields.},
  author    = {Guo Haixiang and Li Yijing and Jennifer Shang and Gu Mingyun and Huang Yuanyue and Gong Bing},
  doi       = {10.1016/J.ESWA.2016.12.035},
  issn      = {0957-4174},
  journal   = {Expert Systems with Applications},
  keywords  = {Data mining,Imbalanced data,Machine learning,Rare events},
  month     = {5},
  pages     = {220-239},
  publisher = {Pergamon},
  title     = {Learning from class-imbalanced data: Review of methods and applications},
  volume    = {73},
  url       = {https://www-sciencedirect-com.ez42.periodicos.capes.gov.br/science/article/pii/S0957417416307175},
  year      = {2017}
}

@article{Mohammed2020,
  abstract  = {Data imbalance in Machine Learning refers to an unequal distribution of classes within a dataset. This issue is encountered mostly in classification tasks in which the distribution of classes or labels in a given dataset is not uniform. The straightforward method to solve this problem is the resampling method by adding records to the minority class or deleting ones from the majority class. In this paper, we have experimented with the two resampling widely adopted techniques: oversampling and undersampling. In order to explore both techniques, we have chosen a public imbalanced dataset from kaggle website Santander Customer Transaction Prediction and have applied a group of well-known machine learning algorithms with different hyperparamters that give best results for both resampling techniques. One of the key findings of this paper is noticing that oversampling performs better than undersampling for different classifiers and obtains higher scores in different evaluation metrics.},
  author    = {Roweida Mohammed and Jumanah Rawashdeh and Malak Abdullah},
  doi       = {10.1109/ICICS49469.2020.239556},
  isbn      = {9781728162270},
  journal   = {2020 11th International Conference on Information and Communication Systems, ICICS 2020},
  keywords  = {Accuracy,Class Imbalance,Machine Learning,Naive Bayes,Oversampling,Precision,Random Forest,Recall,SVM,Undersampling},
  month     = {4},
  pages     = {243-248},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {Machine Learning with Oversampling and Undersampling Techniques: Overview Study and Experimental Results},
  year      = {2020}
}

@article{Yang2024,
  abstract  = {Background: There is currently no consensus on the impact of class imbalance methods on the performance of clinical prediction models. We aimed to empirically investigate the impact of random oversampling and random undersampling, two commonly used class imbalance methods, on the internal and external validation performance of prediction models developed using observational health data. Methods: We developed and externally validated prediction models for various outcomes of interest within a target population of people with pharmaceutically treated depression across four large observational health databases. We used three different classifiers (lasso logistic regression, random forest, XGBoost) and varied the target imbalance ratio. We evaluated the impact on model performance in terms of discrimination and calibration. Discrimination was assessed using the area under the receiver operating characteristic curve (AUROC) and calibration was assessed using calibration plots. Results: We developed and externally validated a total of 1,566 prediction models. On internal and external validation, random oversampling and random undersampling generally did not result in higher AUROCs. Moreover, we found overestimated risks, although this miscalibration could largely be corrected by recalibrating the models towards the imbalance ratios in the original dataset. Conclusions: Overall, we found that random oversampling or random undersampling generally does not improve the internal and external validation performance of prediction models developed in large observational health databases. Based on our findings, we do not recommend applying random oversampling or random undersampling when developing prediction models in large observational health databases.},
  author    = {Cynthia Yang and Egill A. Fridgeirsson and Jan A. Kors and Jenna M. Reps and Peter R. Rijnbeek},
  doi       = {10.1186/S40537-023-00857-7/FIGURES/6},
  issn      = {21961115},
  issue     = {1},
  journal   = {Journal of Big Data},
  keywords  = {Class Imbalance Problem,Clinical decision support,Clinical prediction model,External validation,Machine learning,Patient-level prediction},
  month     = {12},
  pages     = {1-17},
  publisher = {Springer Nature},
  title     = {Impact of random oversampling and random undersampling on the performance of prediction models developed using observational health data},
  volume    = {11},
  url       = {https://link.springer.com/articles/10.1186/s40537-023-00857-7 https://link.springer.com/article/10.1186/s40537-023-00857-7},
  year      = {2024}
}

@article{Fernndez2018,
  abstract  = {The Synthetic Minority Oversampling Technique (SMOTE) preprocessing algorithm is considered "de facto" standard in the framework of learning from imbalanced data. This is due to its simplicity in the design of the procedure, as well as its robustness when applied to different type of problems. Since its publication in 2002, SMOTE has proven successful in a variety of applications from several different domains. SMOTE has also inspired several approaches to counter the issue of class imbalance, and has also significantly contributed to new supervised learning paradigms, including multilabel classification, incremental learning, semi-supervised learning, multi-instance learning, among others. It is standard benchmark for learning from imbalanced data. It is also featured in a number of different software packages - from open source to commercial. In this paper, marking the fifteen year anniversary of SMOTE, we reflect on the SMOTE journey, discuss the current state of affairs with SMOTE, its applications, and also identify the next set of challenges to extend SMOTE for Big Data problems.},
  author    = {Alberto Fernández and Salvador García and Francisco Herrera and Nitesh V. Chawla},
  doi       = {10.1613/JAIR.1.11192},
  issn      = {1076-9757},
  journal   = {Journal of Artificial Intelligence Research},
  month     = {4},
  pages     = {863-905},
  publisher = {AI Access Foundation},
  title     = {SMOTE for Learning from Imbalanced Data: Progress and Challenges, Marking the 15-year Anniversary},
  volume    = {61},
  url       = {https://www.jair.org/index.php/jair/article/view/11192},
  year      = {2018}
}

@article{Chawla2002,
  abstract  = {An approach to the construction of classifiers from    imbalanced datasets is described. A dataset is imbalanced if the    classification categories are not approximately equally    represented. Often real-world data sets are predominately composed of    ``normal'' examples with only a small percentage of ``abnormal'' or    ``interesting'' examples. It is also the case that the cost of    misclassifying an abnormal (interesting) example as a normal example    is often much higher than the cost of the reverse    error. Under-sampling of the majority (normal) class has been proposed    as a good means of increasing the sensitivity of a classifier to the    minority class. This paper shows that a combination of our method of    over-sampling the minority (abnormal) class and under-sampling the    majority (normal) class can achieve better classifier performance (in    ROC space) than only under-sampling the majority class.  This paper    also shows that a combination of our method of over-sampling the    minority class and under-sampling the majority class can achieve    better classifier performance (in ROC space) than varying the loss    ratios in Ripper or class priors in Naive Bayes. Our method of    over-sampling the minority class involves creating synthetic minority    class examples.  Experiments are performed using C4.5, Ripper and a    Naive Bayes classifier. The method is evaluated using the area under    the Receiver Operating Characteristic curve (AUC) and the ROC convex    hull strategy.},
  author    = {Nitesh V. Chawla and Kevin W. Bowyer and Lawrence O. Hall and W. Philip Kegelmeyer},
  doi       = {10.1613/JAIR.953},
  issn      = {1076-9757},
  journal   = {Journal of Artificial Intelligence Research},
  month     = {6},
  pages     = {321-357},
  publisher = {American Association for Artificial Intelligence},
  title     = {SMOTE: Synthetic Minority Over-sampling Technique},
  volume    = {16},
  url       = {https://www.jair.org/index.php/jair/article/view/10302},
  year      = {2002}
}

@article{Wei2025,
  abstract  = {Against the backdrop of dynamic transformations in the financial sector and prominent corporate diversification trends, credit risk prediction becomes significantly more challenging. On one hand, this study focuses on optimizing the Synthetic Minority Over-Sampling Technique (SMOTE) algorithm for corporate credit risk prediction, thereby enhancing financial institutions’ risk management capabilities. The study systematically examines corporate diversification strategies, revealing discrepancies between theoretical frameworks and practical implementations. These strategies complicate corporate financial structures, generating divergent profitability, capital allocation, and risk profiles across business units. Such heterogeneity leads to uneven resource distribution, ultimately impacting enterprise operations, debt servicing capacity, and credit performance. Consequently, financial institutions increasingly prioritize cross-sectoral risk monitoring, business synergy evaluation, and dynamic financial tracking. On the other hand, regarding algorithmic innovation, this study conducts an in-depth analysis of SMOTE’s fundamental principles, encompassing its sample generation mechanics and optimized variants. Especially in terms of optimization content, this study innovatively introduces an adaptive boundary adjustment mechanism that automatically defines minority class boundaries based on data distribution characteristics. Meanwhile, it precisely targets critical oversampling areas while avoiding arbitrary sample generation in irrelevant regions. Moreover, an optimized weight allocation protocol during synthetic sample creation incorporates feature relevance and class distribution to produce more representative new samples. The experimental framework utilizes four benchmark datasets: German Credit, Australian Credit Approval, Taiwan Credit Card Default, and Corporate Credit Risk Assessment. The rigorous methodology ensures research validity through scientific data partitioning, appropriate hardware configuration, an advanced software environment, and comprehensive evaluation indices (accuracy, precision, recall, F1-score). The study mainly focuses on two aspects. One is to analyze corporate diversification strategy; the other is to explore the optimization of the SMOTE algorithm and its application in credit risk prediction. Empirical results demonstrate the optimized SMOTE algorithm’s superiority over six comparison models, such as random over-sampling, under-sampling, etc. The accuracy rate is improved by more than 21%, and the highest is close to 38%. Precision is enhanced by over 28%, peak nearly 35%; Recall is increased by more than 31% and peaked at 42%; F1 score is boosted by approximately 33%, with a maximum of about 39%. This study provides financial institutions with an advanced algorithmic solution for credit risk assessment in diversified corporate environments. Also, it is expected to improve decision-making accuracy, strengthen risk resilience, and promote financial market stability.},
  author    = {Han Wei},
  doi       = {10.1038/S41598-025-09173-X;SUBJMETA=1041,1042,531,639,705;KWRD=APPLIED+MATHEMATICS,COMPUTATIONAL+SCIENCE,STATISTICS},
  issn      = {20452322},
  issue     = {1},
  journal   = {Scientific Reports},
  keywords  = {Corporate diversification,Credit risk,Financial sector,Risk monitoring,SMOTE algorithm},
  month     = {12},
  pages     = {1-18},
  publisher = {Nature Research},
  title     = {SMOTE algorithm optimization and application in corporate credit risk prediction with diversification strategy consideration},
  volume    = {15},
  url       = {https://www-nature-com.ez42.periodicos.capes.gov.br/articles/s41598-025-09173-x},
  year      = {2025}
}

@article{Batista2004,
  abstract  = {There are several aspects that might influence the performance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbalance in which examples ...},
  author    = {Gustavo E. A. P. A. Batista and Ronaldo C. Prati and Maria Carolina Monard},
  doi       = {10.1145/1007730.1007735},
  issn      = {1931-0145},
  issue     = {1},
  journal   = {ACM SIGKDD Explorations Newsletter},
  month     = {6},
  pages     = {20-29},
  publisher = {ACMPUB27New York, NY, USA},
  title     = {A study of the behavior of several methods for balancing machine learning training data},
  volume    = {6},
  url       = {/doi/pdf/10.1145/1007730.1007735?download=true},
  year      = {2004}
}

@misc{Arichandrapandian2024,
  author = {Thangaselvi Arichandrapandian},
  month  = {5},
  title  = {Upsampling And Downsampling: Correcting The Imbalances In Data - Open Source For You},
  url    = {https://www.opensourceforu.com/2024/05/upsampling-and-downsampling-correcting-the-imbalances-in-data-2/},
  year   = {2024}
}

@article{Pereira2020,
  abstract  = {A large variety of problems are multi-labeled, which made the Multi-Label Classification field become an active topic in the machine learning community. However, real world problems tend to be imbalanced, meaning that some classes may have more samples than others. Learning from imbalanced datasets is a challenging task and for that has attracted the attention of researchers that have proposed some resampling algorithms to address this problem. This work presents two main contributions: A new resampling algorithm for multi-label classification problems named MLTL - Multi-Label Tomek Link, which is based on the standard Tomek Link resampling algorithm; A multi-label imbalanceness API for the Mulan framework. Results in seven well-known datasets showed that MLTL is a competitive technique when compared to other multi-label resampling methods from the literature.},
  author    = {Rodolfo M. Pereira and Yandre M.G. Costa and Carlos N. Silla},
  doi       = {10.1016/J.NEUCOM.2019.11.076},
  issn      = {0925-2312},
  journal   = {Neurocomputing},
  keywords  = {Dataset imbalanceness,Multi-label learning,Resampling techniques},
  month     = {3},
  pages     = {95-105},
  publisher = {Elsevier},
  title     = {MLTL: A multi-label approach for the Tomek Link undersampling algorithm},
  volume    = {383},
  url       = {https://www.sciencedirect.com/science/article/abs/pii/S0925231219316790},
  year      = {2020}
}

@article{He2008,
  abstract = {This paper presents a novel adaptive synthetic (ADASYN) sampling approach for learning from imbalanced data sets. The essential idea of ADASYN is to use a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. As a result, the ADASYN approach improves learning with respect to the data distributions in two ways: (1) reducing the bias introduced by the class imbalance, and (2) adaptively shifting the classification decision boundary toward the difficult examples. Simulation analyses on several machine learning data sets show the effectiveness of this method across five evaluation metrics. ©2008 IEEE.},
  author   = {Haibo He and Yang Bai and Edwardo A. Garcia and Shutao Li},
  doi      = {10.1109/IJCNN.2008.4633969},
  isbn     = {9781424418213},
  journal  = {Proceedings of the International Joint Conference on Neural Networks},
  pages    = {1322-1328},
  title    = {ADASYN: Adaptive synthetic sampling approach for imbalanced learning},
  year     = {2008}
}

@article{FernndezCs2018,
  abstract  = {Cost-sensitive learning is an aspect of algorithm-level modifications for class imbalance. Here, instead of using a standard error-driven evaluation (or 0–1 loss function), a misclassification cost is being introduced in order to minimize the conditional risk....},
  author    = {Alberto Fernández and Salvador García and Mikel Galar and Ronaldo C. Prati and Bartosz Krawczyk and Francisco Herrera},
  doi       = {10.1007/978-3-319-98074-4_4},
  isbn      = {978-3-319-98074-4},
  journal   = {Learning from Imbalanced Data Sets},
  pages     = {63-78},
  publisher = {Springer, Cham},
  title     = {Cost-Sensitive Learning},
  url       = {https://link-springer-com.ez42.periodicos.capes.gov.br/chapter/10.1007/978-3-319-98074-4_4},
  year      = {2018}
}

@article{Zhou2016,
  abstract  = {Feature selection aims to select a small subset of informative features that contain most of the information related to a given task. Existing feature selection methods often assume that all the features have the same cost. However, in many real world applications, different features may have different costs (e.g., different tests a patient might take in medical diagnosis). Ignoring the feature cost may produce good feature subsets in theory but they can not be used in practice. In this paper, we propose a random forest-based feature selection algorithm that incorporates the feature cost into the base decision tree construction process to produce low-cost feature subsets. In particular, when constructing a base tree, a feature is randomly selected with a probability inversely proportional to its associated cost. We evaluate the proposed method on a number of UCI datasets and apply it to a medical diagnosis problem where the real feature costs are estimated by experts. The experimental results demonstrate that our feature-cost-sensitive random forest (FCS-RF) is able to select a low-cost subset of informative features and achieves better performance than other state-of-art feature selection methods in real-world problems.},
  author    = {Qifeng Zhou and Hao Zhou and Tao Li},
  doi       = {10.1016/J.KNOSYS.2015.11.010},
  issn      = {0950-7051},
  journal   = {Knowledge-Based Systems},
  keywords  = {Cost sensitive,Feature selection,Random forest},
  month     = {3},
  pages     = {1-11},
  publisher = {Elsevier},
  title     = {Cost-sensitive feature selection using random forest: Selecting low-cost subsets of informative features},
  volume    = {95},
  url       = {https://www-sciencedirect-com.ez42.periodicos.capes.gov.br/science/article/pii/S0950705115004372},
  year      = {2016}
}
@article{Krawczyk2016,
  abstract  = {Despite more than two decades of continuous development learning from imbalanced data is still a focus of intense research. Starting as a problem of skewed distributions of binary tasks, this topic evolved way beyond this conception. With the expansion of machine learning and data mining, combined with the arrival of big data era, we have gained a deeper insight into the nature of imbalanced learning, while at the same time facing new emerging challenges. Data-level and algorithm-level methods are constantly being improved and hybrid approaches gain increasing popularity. Recent trends focus on analyzing not only the disproportion between classes, but also other difficulties embedded in the nature of data. New real-life problems motivate researchers to focus on computationally efficient, adaptive and real-time methods. This paper aims at discussing open issues and challenges that need to be addressed to further develop the field of imbalanced learning. Seven vital areas of research in this topic are identified, covering the full spectrum of learning from imbalanced data: classification, regression, clustering, data streams, big data analytics and applications, e.g., in social media and computer vision. This paper provides a discussion and suggestions concerning lines of future research for each of them.},
  author    = {Bartosz Krawczyk},
  doi       = {10.1007/S13748-016-0094-0/TABLES/1},
  issn      = {21926360},
  issue     = {4},
  journal   = {Progress in Artificial Intelligence},
  keywords  = {Big data,Data streams,Imbalanced clustering,Imbalanced data,Imbalanced regression,Machine learning,Multi-class imbalance},
  month     = {11},
  pages     = {221-232},
  publisher = {Springer Verlag},
  title     = {Learning from imbalanced data: open challenges and future directions},
  volume    = {5},
  url       = {https://link-springer-com.ez42.periodicos.capes.gov.br/article/10.1007/s13748-016-0094-0},
  year      = {2016}
}

@article{Landgrebe2004,
  abstract  = {A common assumption made in the field of Pattern Recognition is that the priors inherent to the class distributions in the training set are representative of the true class distributions. However this assumption does not always hold, since the true...},
  author    = {Thomas Landgrebe and Pavel Paclík and David M.J. Tax and Serguei Verzakov and Robert P.W. Duin},
  doi       = {10.1007/978-3-540-27868-9_83},
  isbn      = {978-3-540-27868-9},
  issn      = {1611-3349},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  pages     = {762-770},
  publisher = {Springer, Berlin, Heidelberg},
  title     = {Cost-Based Classifier Evaluation for Imbalanced Problems},
  volume    = {3138},
  url       = {https://link-springer-com.ez42.periodicos.capes.gov.br/chapter/10.1007/978-3-540-27868-9_83},
  year      = {2004}
}

@article{Ling2006,
  abstract = {In medical diagnosis, doctors must often determine what medical tests (e.g., X-ray and blood tests) should be ordered for a patient to minimize the total cost of medical tests and misdiagnosis. In this paper, we design cost-sensitive machine learning algorithms to model this learning and diagnosis process. Medical tests are like attributes in machine learning whose values may be obtained at a cost (attribute cost), and misdiagnoses are like misclassifications which may also incur a cost (misclassification cost). We first propose a lazy decision tree learning algorithm that minimizes the sum of attribute costs and misclassification costs. Then, we design several novel "test strategies" that can request to obtain values of unknown attributes at a cost (similar to doctors' ordering of medical tests at a cost) in order to minimize the total cost for test examples (new patients). These test strategies correspond to different situations in real-world diagnoses. We empirically evaluate these test strategies, and show that they are effective and outperform previous methods. Our results can be readily applied to real-world diagnosis tasks. A case study on heart disease is given throughout the paper. © 2006 IEEE.},
  author   = {Charles X. Ling and Victor S. Sheng and Qiang Yang},
  doi      = {10.1109/TKDE.2006.131},
  issn     = {10414347},
  issue    = {8},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  keywords = {Classification,Concept learning,Induction,Mining methods and algorithms},
  month    = {8},
  pages    = {1055-1067},
  title    = {Test strategies for cost-sensitive decision trees},
  volume   = {18},
  year     = {2006}
}

@article{Sahin2013,
  abstract  = {With the developments in the information technology, fraud is spreading all over the world, resulting in huge financial losses. Though fraud prevention mechanisms such as CHIP&PIN are developed for credit card systems, these mechanisms do not prevent the most common fraud types such as fraudulent credit card usages over virtual POS (Point Of Sale) terminals or mail orders so called online credit card fraud. As a result, fraud detection becomes the essential tool and probably the best way to stop such fraud types. In this study, a new cost-sensitive decision tree approach which minimizes the sum of misclassification costs while selecting the splitting attribute at each non-terminal node is developed and the performance of this approach is compared with the well-known traditional classification models on a real world credit card data set. In this approach, misclassification costs are taken as varying. The results show that this cost-sensitive decision tree algorithm outperforms the existing well-known methods on the given problem set with respect to the well-known performance metrics such as accuracy and true positive rate, but also a newly defined cost-sensitive metric specific to credit card fraud detection domain. Accordingly, financial losses due to fraudulent transactions can be decreased more by the implementation of this approach in fraud detection systems. © 2013 Elsevier Ltd. All rights reserved.},
  author    = {Yusuf Sahin and Serol Bulkan and Ekrem Duman},
  doi       = {10.1016/J.ESWA.2013.05.021},
  issn      = {0957-4174},
  issue     = {15},
  journal   = {Expert Systems with Applications},
  keywords  = {Classification,Cost-sensitive modeling,Credit card fraud detection,Decision tree induction,Variable misclassification cost},
  month     = {11},
  pages     = {5916-5923},
  publisher = {Pergamon},
  title     = {A cost-sensitive decision tree approach for fraud detection},
  volume    = {40},
  url       = {https://www-sciencedirect-com.ez42.periodicos.capes.gov.br/science/article/pii/S0957417413003072},
  year      = {2013}
}

@article{Bahnsen2015,
  abstract  = {Several real-world classification problems are example-dependent cost-sensitive in nature, where the costs due to misclassification vary between examples. However, standard classification methods do not take these costs into account, and assume a constant cost of misclassification errors. State-of-the-art example-dependent cost-sensitive techniques only introduce the cost to the algorithm, either before or after training, therefore, leaving opportunities to investigate the potential impact of algorithms that take into account the real financial example-dependent costs during an algorithm training. In this paper, we propose an example-dependent cost-sensitive decision tree algorithm, by incorporating the different example-dependent costs into a new cost-based impurity measure and a new cost-based pruning criteria. Then, using three different databases, from three real-world applications: credit card fraud detection, credit scoring and direct marketing, we evaluate the proposed method. The results show that the proposed algorithm is the best performing method for all databases. Furthermore, when compared against a standard decision tree, our method builds significantly smaller trees in only a fifth of the time, while having a superior performance measured by cost savings, leading to a method that not only has more business-oriented results, but also a method that creates simpler models that are easier to analyze.},
  author    = {Alejandro Correa Bahnsen and Djamila Aouada and Björn Ottersten},
  doi       = {10.1016/J.ESWA.2015.04.042},
  issn      = {0957-4174},
  issue     = {19},
  journal   = {Expert Systems with Applications},
  keywords  = {Cost-sensitive classifier,Cost-sensitive learning,Credit scoring,Decision trees,Direct marketing,Fraud detection},
  month     = {11},
  pages     = {6609-6619},
  publisher = {Pergamon},
  title     = {Example-dependent cost-sensitive decision trees},
  volume    = {42},
  url       = {https://www-sciencedirect-com.ez42.periodicos.capes.gov.br/science/article/pii/S0957417415002845},
  year      = {2015}
}

@article{Domingos1999,
  abstract  = {Research in machine learning, statistics and related fields has produced a wide variety of algorithms for classification. However, most of these algorithms assume that all errors have the same cost, which is seldom the case in KDD prob- lems. Individually making each classification learner costsensitive is laborious, and often non-trivial. In this paper we propose a principled method for making an arbitrary classifier cost-sensitive by wrapping a cost-minimizing procedure around it. This procedure, called MetaCost, treats the underlying classifier as a black box, requiring no knowledge of its functioning or change to it. Unlike stratification, MetaCost is applicable to any number of classes and to arbitrary cost matrices. Empirical trials on a large suite of benchmark databases show that MetaCost almost always produces large cost reductions compared to the cost-blind classifier used (C4.5RULES) and to two forms of stratification. Further tests identify the key components of MetaCost and those that can be varied without substantial loss. Experiments on a larger database indicate that MetaCost scales well.},
  author    = {Pedro Domingos},
  doi       = {10.1145/312129.312220},
  month     = {8},
  pages     = {155-164},
  publisher = {Association for Computing Machinery (ACM)},
  title     = {Metacost: A general method for making classifiers cost-sensitive},
  url       = {https://dl-acm-org.ez42.periodicos.capes.gov.br/doi/pdf/10.1145/312129.312220},
  year      = {1999}
}

@article{Wang2018,
  abstract  = {In credit rating, borrowers are classified into some grades, based on their past credit experience, to recognize potential borrowers who are of different probability of default. Classification algorithms are commonly used, which is important to reduce loss for banks or investors. Since multiple classes are imbalanced, as well as losses of misclassification across classes are not uniform, multi-class cost-sensitive classifiers should be paid more attention in credit rating. Based on existing literatures, in this paper, we review several cost-sensitive classifiers and compare them with three assumed cost matrices. The empirical study utilizes data collected from Lending Club which is the largest U.S. P2P loan platform. The results are aimed at giving insights on cost-sensitive classifiers with various performances in credit rating, which are concerned by researchers and practitioners of credit lending.},
  author    = {Haomin Wang and Gang Kou and Yi Peng},
  doi       = {10.1109/ICCCC.2018.8390460},
  isbn      = {9781538619346},
  journal   = {2018 7th International Conference on Computers Communications and Control, ICCCC 2018 - Proceedings},
  keywords  = {P2P lending,classification algorithm,cost-sensitive learning,credit rating,data mining},
  month     = {6},
  pages     = {210-213},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {Cost-sensitive classifiers in credit rating: A comparative study on P2P lending},
  year      = {2018}
}

@article{Araf2024,
  abstract  = {Integrating Machine Learning (ML) in medicine has unlocked many opportunities to harness complex medical data, enhancing patient outcomes and advancing the field. However, the inherent imbalanced distribution of medical data poses a significant challenge, resulting in biased ML models that perform poorly on minority classes. Mitigating the impact of class imbalance has prompted researchers to explore various strategies, wherein Cost-Sensitive Learning (CSL) arises as a promising approach to improve the accuracy and reliability of ML models. This paper presents the first review of CSL for imbalanced medical data. A comprehensive exploration of the existing literature encompassed papers published from January 2010 to December 2022 and sourced from five major digital libraries. A total of 173 papers were selected, analysed, and classified based on key criteria, including publication years, channels and sources, research types, empirical types, medical sub-fields, medical tasks, CSL approaches, strengths and weaknesses of CSL, frequently used datasets and data types, evaluation metrics, and development tools. The results indicate a noteworthy publication rise, particularly since 2020, and a strong preference for CSL direct approaches. Data type analysis unveiled diverse modalities, with medical images prevailing. The underutilisation of cost-related metrics and the prevalence of Python as the primary programming tool are highlighted. The strengths and weaknesses analysis covered three aspects: CSL strategy, CSL approaches, and relevant works. This study serves as a valuable resource for researchers seeking to explore the current state of research, identify strengths and gaps in the existing literature and advance CSL’s application for imbalanced medical data.},
  author    = {Imane Araf and Ali Idri and Ikram Chairi},
  doi       = {10.1007/S10462-023-10652-8},
  isbn      = {0123456789},
  issn      = {1573-7462},
  issue     = {4},
  journal   = {Artificial Intelligence Review 2024 57:4},
  keywords  = {Artificial Intelligence,Computer Science,Medical data,general},
  month     = {3},
  pages     = {1-72},
  publisher = {Springer},
  title     = {Cost-sensitive learning for imbalanced medical data: a review},
  volume    = {57},
  url       = {https://link-springer-com.ez42.periodicos.capes.gov.br/article/10.1007/s10462-023-10652-8},
  year      = {2024}
}

@article{Chen2021,
  abstract  = {This paper is concerned with the class imbalance problem which has been known to hinder the learning performance of classification algorithms. The problem occurs when there are significantly more training samples available for some classes than for others. Various real-world datasets suffer from this phenomenon. The cost-sensitive classification algorithm has shown good classification performance on imbalanced datasets. Besides, active learning has shown potential for solving the class imbalance problem by providing the classifier more balanced datasets. In this paper, we propose a novel hybrid algorithm combining active learning and cost sensitive classification within the Metacost framework. Based on the selection strategies in active learning, our method can construct more informative balanced datasets for each base classifier in the Metacost framework, which can improve the classification performance. Our experimental results show that the proposed algorithm can achieve more competitive prediction performance in terms of Recall, G_mean and Area Under roc Curve.},
  author    = {You Chen},
  doi       = {10.1109/CAIBDA53561.2021.00054},
  isbn      = {9781665424905},
  journal   = {Proceedings - 2021 International Conference on Artificial Intelligence, Big Data and Algorithms, CAIBDA 2021},
  keywords  = {Metacost,classification,cost-sensitive,imbalance},
  month     = {5},
  pages     = {224-228},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {Research on Cost-sensitive Classification Methods for Imbalanced Data},
  year      = {2021}
}

@misc{Shi2022,
  abstract  = {Credit risk assessment is at the core of modern economies. Traditionally, it is measured by statistical methods and manual auditing. Recent advances in financial artificial intelligence stemmed from a new wave of machine learning (ML)-driven credit risk models that gained tremendous attention from both industry and academia. In this paper, we systematically review a series of major research contributions (76 papers) over the past eight years using statistical, machine learning and deep learning techniques to address the problems of credit risk. Specifically, we propose a novel classification methodology for ML-driven credit risk algorithms and their performance ranking using public datasets. We further discuss the challenges including data imbalance, dataset inconsistency, model transparency, and inadequate utilization of deep learning models. The results of our review show that: 1) most deep learning models outperform classic machine learning and statistical algorithms in credit risk estimation, and 2) ensemble methods provide higher accuracy compared with single models. Finally, we present summary tables in terms of datasets and proposed models.},
  author    = {Si Shi and Rita Tse and Wuman Luo and Stefano D’Addona and Giovanni Pau},
  doi       = {10.1007/s00521-022-07472-2},
  issn      = {14333058},
  issue     = {17},
  journal   = {Neural Computing and Applications},
  keywords  = {Credit risk,Deep learning,Machine learning,Statistical learning},
  month     = {9},
  pages     = {14327-14339},
  publisher = {Springer Science and Business Media Deutschland GmbH},
  title     = {Machine learning-driven credit risk: a systemic review},
  volume    = {34},
  year      = {2022}
}

@article{Christiano2008,
  abstract = {The United States is indisputably undergoing a Önancial crisis and is perhaps headed for a deeprecession. Here we examine three claims about the way the Önancial crisis is afecting the economy as a whole and argue that all three claims are myths. We also present three underappreciated facts about how the Önancial system intermediates funds between households and corporate businesses.Conventional analyses of the Önancial crisis focus on interest rate spreads. We argue that such analyses may lead to mistaken inferences about the real costs of borrowing and argue that, during Önancial crises, variations in the levels of nominal interest rates might lead to better inferences about variations in the real costs of borrowing. Moreover, we argue that even if current increase in spreads indicate increases in the riskiness of the underlying projects, by itself, this increase does not necessarily indicate the need for massive government intervention. We call for policymakers to articulate the precise nature of the market failure they see, to present hard evidence that di§erentiates their view of the data from other views which would not require such intervention, and to share with the public the logic and evidence that burnishes the case that the particular intervention they are advocating will Öx this market failure.},
  author   = {Lawrence Christiano and Patrick Kehoe},
  journal  = {researchgate.net},
  month    = {10},
  title    = {Facts and Myths about the Financial Crisis of 2008},
  url      = {https://www.researchgate.net/profile/Lawrence-Christiano/publication/23529752_Technical_Notes_on_Facts_and_Myths_about_the_Financial_Crisis_of_2008/links/00b4953b4162728952000000/Technical-Notes-on-Facts-and-Myths-about-the-Financial-Crisis-of-2008.pdf},
  year     = {2008}
}

@article{Deng2017,
  abstract  = {Can we train the computer to beat experienced traders for financial assert trading? In this paper, we try to address this challenge by introducing a recurrent deep neural network (NN) for real-time financial signal representation and trading. Our model is inspired by two biological-related learning concepts of deep learning (DL) and reinforcement learning (RL). In the framework, the DL part automatically senses the dynamic market condition for informative feature learning. Then, the RL module interacts with deep representations and makes trading decisions to accumulate the ultimate rewards in an unknown environment. The learning system is implemented in a complex NN that exhibits both the deep and recurrent structures. Hence, we propose a task-aware backpropagation through time method to cope with the gradient vanishing issue in deep training. The robustness of the neural system is verified on both the stock and the commodity future markets under broad testing conditions.},
  author    = {Yue Deng and Feng Bao and Youyong Kong and Zhiquan Ren and Qionghai Dai},
  doi       = {10.1109/TNNLS.2016.2522401},
  issn      = {21622388},
  issue     = {3},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  keywords  = {Deep learning (DL),Reinforcement learning (RL),financial signal processing,neural network (NN) for finance},
  month     = {3},
  pages     = {653-664},
  pmid      = {26890927},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {Deep Direct Reinforcement Learning for Financial Signal Representation and Trading},
  volume    = {28},
  url       = {https://ieeexplore-ieee-org.ez42.periodicos.capes.gov.br/abstract/document/7407387},
  year      = {2017}
}

@article{Pedregosa2011scikit,
  title     = {Scikit-learn: Machine learning in Python},
  author    = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal   = {the Journal of machine Learning research},
  volume    = {12},
  pages     = {2825--2830},
  year      = {2011},
  publisher = {JMLR. org}
}

@article{Iranmehr2019,
  abstract  = {Many machine learning applications involve imbalance class prior probabilities, multi-class classification with many classes (often addressed by one-versus-rest strategy), or “cost-sensitive” classification. In such domains, each class (or in some cases, each sample) requires special treatment. In this paper, we use a constructive procedure to extend SVM's standard loss function to optimize the classifier with respect to class imbalance or class costs. By drawing connections between risk minimization and probability elicitation, we show that the resulting classifier guarantees Bayes consistency. We further analyze the primal and the dual objective functions and derive the objective function in a regularized risk minimization framework. Finally, we extend the classifier to the with cost-sensitive learning with example dependent costs. We perform experimental analysis on class imbalance, cost-sensitive learning with given class and example costs and show that proposed algorithm provides superior generalization performance, compared to conventional methods.},
  author    = {Arya Iranmehr and Hamed Masnadi-Shirazi and Nuno Vasconcelos},
  doi       = {10.1016/J.NEUCOM.2018.11.099},
  issn      = {0925-2312},
  journal   = {Neurocomputing},
  keywords  = {Bayes consistency,Class imbalance,Classification,Cost-sensitive learning,SVM},
  month     = {5},
  pages     = {50-64},
  publisher = {Elsevier},
  title     = {Cost-sensitive support vector machines},
  volume    = {343},
  url       = {https://www-sciencedirect-com.ez42.periodicos.capes.gov.br/science/article/pii/S0925231219301614},
  year      = {2019}
}

@article{Cao2013,
  abstract  = {Class imbalance is one of the challenging problems for machine learning in many real-world applications. Cost-sensitive learning has attracted significant attention in recent years to solve the problem, but it is difficult to determine the precise misclassification...},
  author    = {Peng Cao and Dazhe Zhao and Osmar Zaiane},
  doi       = {10.1007/978-3-642-37456-2_24},
  isbn      = {978-3-642-37456-2},
  issn      = {1611-3349},
  issue     = {PART 2},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  pages     = {280-292},
  publisher = {Springer, Berlin, Heidelberg},
  title     = {An Optimized Cost-Sensitive SVM for Imbalanced Data Learning},
  volume    = {7819 LNAI},
  url       = {https://link-springer-com.ez42.periodicos.capes.gov.br/chapter/10.1007/978-3-642-37456-2_24},
  year      = {2013}
}
